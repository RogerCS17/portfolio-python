{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKpNaxLeRFWtRFkY5bLoHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogerCS17/portfolio-python/blob/main/Big%20Data/Spark/exercises_spark04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspectos Avanzados sobre RDD"
      ],
      "metadata": {
        "id": "uLcKTuC7T-vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Almacenamiento en caché"
      ],
      "metadata": {
        "id": "Bj7EGE0KUGnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El almacenamiento en caché permite que Spark conserve los datos en todos los cálculos y operaciones.\n",
        "\n",
        "- Se usa persist( ) o cache( ) para almacenarlo en caché\n",
        "- cache( ) es simplemente un sinónimo de persist(MEMORY_ONLY)"
      ],
      "metadata": {
        "id": "qlNHKAfHVmeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Zi9HGOT3ZK",
        "outputId": "4785def1-20e9-47f1-8328-04aea817519c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Se instala PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Se importa pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Se crea una sesión\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Se crea un SparkContext\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea un RDD\n",
        "rdd = sc.parallelize([item for item in range(10)])\n",
        "\n",
        "# Se muestra el resultado\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAv1LpvgUoas",
        "outputId": "71520f4d-3898-404b-eac0-00f62afa635f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importa StorageLevel\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Se escoge el nivel de persistencia del RDD\n",
        "rdd.persist(StorageLevel.MEMORY_ONLY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG3pOgM8U7e2",
        "outputId": "d6838b07-ad11-45b2-ea01-3748f88ed411"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si se quiere cambiar el nivel de persistencia, se debe usar unpersist\n",
        "rdd.unpersist()\n",
        "\n",
        "# Ahora se le asigna un nuevo nivel de persistencia\n",
        "rdd.persist(StorageLevel.DISK_ONLY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgdI4-OYVPvn",
        "outputId": "e98a2714-f62e-4050-8f3a-d13061e373fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particionado"
      ],
      "metadata": {
        "id": "202hz-tWWs0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los RDD operan con datos no como una sola masa de datos, sino que administran y operan los datos en particiones repartidas por todo el clúster\n",
        "- El número de particiones es importante\n",
        "\n",
        "Particionadores\n",
        "- HashPartitioner: Es el particionador predeterminado en Spark.\n",
        "\n",
        "  Fórmula: partitionIndex = hash(key) % numPartitions\n",
        "- RangePartitioner: Funciona dividiendo el RDD en rangos aproximadamente iguales\n"
      ],
      "metadata": {
        "id": "Lx-r-uO6W04J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el rdd\n",
        "rdd = sc.parallelize([\"x\", \"y\", \"z\"])\n",
        "\n",
        "# Se establece el número de particiones\n",
        "num_partitions = 6\n",
        "\n",
        "# Se aplica la fórmula\n",
        "hash(\"z\") % num_partitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfjZKNRnXyrq",
        "outputId": "9b69695b-c511-4319-f02c-72f4ac62402b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcast Variables\n",
        "Las variables broadcast son variables compartidas entre todos los ejecutores. Estos se crean una vez en el controlador y luego se leen sólo en los ejecutores"
      ],
      "metadata": {
        "id": "jiRY0yi6bOy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el RDD\n",
        "rdd = sc.parallelize([item for item in range(10)])\n",
        "\n",
        "# Se muestra el resultado\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtkYd3RnbLgQ",
        "outputId": "aff3c34f-5c68-4b26-d7c6-42620860dfca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el broadcast\n",
        "number = 1\n",
        "br_number = sc.broadcast(number)\n",
        "\n",
        "# Se crea otro rdd\n",
        "rdd_number = rdd.map(lambda x: x + br_number.value)\n",
        "\n",
        "# Se muestra el resultado\n",
        "rdd_number.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHIgSUGscPiy",
        "outputId": "2daa8fad-375b-41fe-a812-199cf0e1431c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acumuladores"
      ],
      "metadata": {
        "id": "4eJ6Pb8si_3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los acumuladores son variables compartidos entre ejecutores que normalmente se utilizan para agregar contadores a su programa Spark.\n",
        "- sparkContext.accumulator( ): Se usa para definir variables de acumulador.\n",
        "- add( ): Se usa para agregar/actualizar un valor en el acumulador.\n",
        "- La propiedad value de la variable del acumulador se utiliza para recuperar el valor del acumulador."
      ],
      "metadata": {
        "id": "gBXP1hgMjB_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el acumulador\n",
        "acc = sc.accumulator(0)\n",
        "\n",
        "# Se crea el RDD\n",
        "rdd = sc.parallelize([2,4,6,8,10])\n",
        "\n",
        "# Se usa el acumulador\n",
        "rdd.foreach(lambda x: acc.add(x))\n",
        "\n",
        "# Se muestra el resultado\n",
        "acc.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVvg_SVsjaPM",
        "outputId": "63fc6595-1bde-4f7c-e1fc-ce5f4d4f406d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios"
      ],
      "metadata": {
        "id": "kNoA-dF-kvLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cree un RDD importes a partir de los datos adjuntos a esta lección como recurso. Emplee acumuladores para obtener el total de ventas realizadas y el importe total de las ventas."
      ],
      "metadata": {
        "id": "0BmLmjWYkzRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se lee el archivo\n",
        "imports = sc.textFile(\"./rdd.txt\")"
      ],
      "metadata": {
        "id": "WqhqUyV7lZib"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define los acumuladores\n",
        "total_sales = sc.accumulator(0)\n",
        "import_total = sc.accumulator(0)\n",
        "\n",
        "# Se calcula la cantidad de ventas totales\n",
        "imports.foreach(lambda x: total_sales.add(1))\n",
        "\n",
        "# Se calcula el import total\n",
        "imports.foreach(lambda x: import_total.add(float(x)))\n",
        "\n",
        "# Se muestra lso resultados\n",
        "print(f\"Se vendio la cantidad total de: {total_sales.value} y la venta total fue de: {import_total.value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYnbDlhflmI3",
        "outputId": "4bc9305c-8d32-4bcc-b6ff-63f3780b0434"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se vendio la cantidad total de: 10000 y la venta total fue de: 5042335.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Si se conoce que a cada venta hay que restarle un importe fijo igual a 10 pesos por temas de impuestos."
      ],
      "metadata": {
        "id": "McQ65y4yk-nU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ¿Cómo restaría este impuesto de cada venta utilizando una variable broadcast para acelerar el proceso?"
      ],
      "metadata": {
        "id": "lbQVZ55tlBiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el broadcast\n",
        "tax = sc.broadcast(10)"
      ],
      "metadata": {
        "id": "qU4IgYvLn2Bx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cree un RDD llamado ventas_sin_impuestos a partir de la propuesta del inciso a que contenga las ventas sin impuestos."
      ],
      "metadata": {
        "id": "yGZeLeVtlGeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el RDD\n",
        "sales_no_tax = imports.map(lambda x: float(x) - tax.value)\n",
        "\n",
        "# Se muestra el resultado\n",
        "sales_no_tax.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIYIsWFIoMHi",
        "outputId": "0cd57aa3-ebb7-4e4d-9adf-e4baad69c320"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[517.0, 376.0, 691.0, 230.0, 931.0]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Destruya la variable broadcast creada luego de emplearla para crear el RDD del inciso b."
      ],
      "metadata": {
        "id": "ICyJu55WlIv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tax.destroy()"
      ],
      "metadata": {
        "id": "LqvxeRXOoezf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Persista el RDD ventas_sin_impuestos en los siguientes niveles de persistencia."
      ],
      "metadata": {
        "id": "OxGDbwTslKYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importa StorageLevel\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# A nivel de memoria\n",
        "sales_no_tax.cache()\n",
        "\n",
        "# Se quita el nivel de persistencia\n",
        "sales_no_tax.unpersist()\n",
        "\n",
        "# A nivel de Disco\n",
        "sales_no_tax.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "# Se quita el nivel de persistencia\n",
        "sales_no_tax.unpersist()\n",
        "\n",
        "# A nivel de memoria y disco\n",
        "sales_no_tax.persist(StorageLevel.MEMORY_AND_DISK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242r2sBelGB6",
        "outputId": "8af7e855-8bc5-4a75-cc3d-f0d5b8781b16"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[11] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}